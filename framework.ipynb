{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_preprocess=True\n",
    "is_coref=True\n",
    "is_char_replace=True\n",
    "\n",
    "basic_units=3\n",
    "\n",
    "DATA_PATH='data/IMDB_movie_details.json.zip'\n",
    "\n",
    "# After event centeric segmetation dataset path\n",
    "OUTPUT1_PATH=f\"data/preprocessed/movie_synopsis_segments_n{basic_units}.pickle\"\n",
    "\n",
    "# After context extraction dataset path\n",
    "OUTPUT2_PATH=f\"data/preprocessed/movie_synopsis_segments_after_n{basic_units}.csv\"\n",
    "\n",
    "# After MAD prediction dataset path\n",
    "OUTPUT3_PATH=\"data/movie_dataset/movie_dataset_all.csv\"\n",
    "\n",
    "# Final CharMoral Dataset with label and split\n",
    "FINAL_PATH=\"data/movie_dataset/movie_dataset_total.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preprocessing\n",
    "\n",
    "1. basic preprocessing \n",
    "2. applying coreference\n",
    "3. character name replacement\n",
    "\n",
    "- If you want to skip this process, download [movie_synopsis_preprocess.csv](https://drive.google.com/drive/folders/1vKi83rjJWo9-Bp2kigcY-GLi6nLAN-8-) and put it into `data/preprocessed/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip dataset\n",
    "with ZipFile(DATA_PATH, 'r') as zipObj:\n",
    "    zipObj.extractall('data/')\n",
    "\n",
    "\n",
    "movie_dataset=pd.read_json(\"data/IMDB_movie_details.json\", lines=True)\n",
    "\n",
    "plot_synopsis=list(movie_dataset[\"plot_synopsis\"])\n",
    "\n",
    "plot_synopsis_len=token_len(plot_synopsis)\n",
    "movie_dataset[\"plot_synopsis_len\"]=plot_synopsis_len\n",
    "\n",
    "# Remove if the length of 'plot_synopsis_len' lower than 10\n",
    "drop_idxs=movie_dataset[movie_dataset[\"plot_synopsis_len\"]<10].index\n",
    "print(\"number of drops: \", len(drop_idxs))\n",
    "movie_dataset=movie_dataset.drop(drop_idxs)\n",
    "movie_dataset=movie_dataset.reset_index(drop=True)\n",
    "\n",
    "# plot_synopsis preprocessing\n",
    "if is_preprocess:\n",
    "    movie_dataset[\"plot_synopsis\"]=preprocess(movie_dataset[\"plot_synopsis\"])\n",
    "\n",
    "# coreference\n",
    "if is_coref:\n",
    "    movie_dataset[\"plot_synopsis_coref\"]=movie_dataset['plot_synopsis'].apply(resolve_coreferences)\n",
    "\n",
    "# character replacement\n",
    "if is_char_replace:\n",
    "    movie_dataset['plot_synopsis_cvt'] = movie_dataset['plot_synopsis_coref'].apply(character_replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Event-Centric Story Segmentation\n",
    "\n",
    "- If you want to skip this process, download [movie_synopsis_segments_n3.pickle](https://drive.google.com/drive/folders/1vKi83rjJWo9-Bp2kigcY-GLi6nLAN-8-) and put it into `data/preprocessed/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict=[]\n",
    "for i in range(len(movie_dataset)):\n",
    "    segment_texts=[]\n",
    "    segment_char=[]\n",
    "    movie_id=movie_dataset['movie_id'][i]\n",
    "    movie_genre=movie_dataset[\"genre\"][i]\n",
    "    text_list=movie_dataset[\"plot_synopsis_cvt\"][i].split(\".\")\n",
    "    \n",
    "    for i in range(0, len(text_list), basic_units):\n",
    "        merged_element = \".\".join(map(str, text_list[i:i + basic_units]))\n",
    "        segment_texts.append(merged_element)\n",
    "    \n",
    "    segment_texts=[x for x in segment_texts if len(x)>10]\n",
    "    for i in range(len(segment_texts)):\n",
    "        segment_char.append(find_pattern_in_text(segment_texts[i]))\n",
    "    \n",
    "    assert len(segment_texts)==len(segment_char)\n",
    "\n",
    "    # main character\n",
    "    character_occurrence=[]\n",
    "    for i in range(len(segment_char)):\n",
    "        character_occurrence.extend(segment_char[i])\n",
    "        segment_char[i]=list(set(segment_char[i]))\n",
    "\n",
    "    main_character=Counter(character_occurrence).most_common(5)\n",
    "    \n",
    "    output_dict.append({\n",
    "        \"movie_id\": movie_id,\n",
    "        \"genre\": movie_genre,\n",
    "        \"segments\": segment_texts,\n",
    "        \"segment_char\": segment_char,\n",
    "        \"main_character\": main_character\n",
    "    })\n",
    "\n",
    "final_dict=[]\n",
    "for i in range(len(output_dict)):\n",
    "    segment_char=output_dict[i][\"segment_char\"]\n",
    "    segments=output_dict[i][\"segments\"]\n",
    "    main_char=output_dict[i][\"main_character\"]\n",
    "\n",
    "    merged_segments, merged_characters = merge_segments_with_characters(segments, segment_char)\n",
    "    \n",
    "    final_dict.append({\n",
    "        \"movie_id\": output_dict[i][\"movie_id\"],\n",
    "        \"genre\": output_dict[i][\"genre\"],\n",
    "        \"segments\": merged_segments,\n",
    "        \"segment_char\": merged_characters,\n",
    "        \"main_char\": main_char\n",
    "    })\n",
    "\n",
    "save_pickle(OUTPUT1_PATH, final_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action Extraction\n",
    "\n",
    "- If you want to skip this process, download [movie_synopsis_segments_after_n3.pickle](https://drive.google.com/drive/folders/1vKi83rjJWo9-Bp2kigcY-GLi6nLAN-8-) and put it into `data/preprocessed/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict=[]\n",
    "loaded_data=load_pickle(OUTPUT1_PATH)\n",
    "for i in tqdm(range(len(loaded_data)), desc='transform',mininterval=0.01):\n",
    "    segments=loaded_data[i]['segments']\n",
    "    segment_char=loaded_data[i]['segment_char']\n",
    "\n",
    "    segment_transform=[]\n",
    "    \n",
    "    for seg, seg_char in zip(segments, segment_char):\n",
    "        output=action_extraction(seg, seg_char)\n",
    "        segment_transform.append(output)\n",
    "    \n",
    "    output_dict.append(segment_transform)\n",
    "    \n",
    "loaded_data['segment_transform']=output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "for i in range(len(loaded_data)):\n",
    "    for j in range(len(loaded_data[i]['segment_transform'])):\n",
    "        tmp=loaded_data[i]['segment_transform'][j]\n",
    "        tmp=tmp.strip('<start>').strip('<end>').replace('\"','').split('\\n')\n",
    "        \n",
    "        for k in range(len(tmp)):\n",
    "            if (len(tmp[k])>2) and (\":\" in tmp[k]):\n",
    "                split=tmp[k].replace(',','').split(':')\n",
    "\n",
    "                character_name=split[0].strip('[').strip(']')\n",
    "                action=\" \".join(split[1:])\n",
    "\n",
    "                output.append({\n",
    "                    'mid': i,\n",
    "                    'sid': j,\n",
    "                    'movie_id': loaded_data[i]['movie_id'],\n",
    "                    'genre': loaded_data[i]['genre'],\n",
    "                    'rating': loaded_data[i]['rating'],\n",
    "                    'plot_synopsis_len': loaded_data[i]['plot_synopsis_len'],\n",
    "                    'plot_synopsis_cvt': loaded_data[i]['plot_synopsis_len'],\n",
    "                    'segment_token_len': len(loaded_data[i]['segments'][j].split(' ')),\n",
    "                    'segment': loaded_data[i]['segments'][j],\n",
    "                    'segment_char': character_name,\n",
    "                    'segment_action': action,\n",
    "                    })\n",
    "\n",
    "output_df=pd.DataFrame(output)\n",
    "output_df['segment_action']=output_df['segment_action'].str.replace('[','').str.replace(']','')\n",
    "output_df=output_df.reset_index(drop=True)\n",
    "\n",
    "output_df.to_csv(OUTPUT2_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Context Extraction\n",
    "\n",
    "- If you want to skip this process, \n",
    "1) download [movie_synopsis_segments_after_n3.pickle](https://drive.google.com/drive/folders/1vKi83rjJWo9-Bp2kigcY-GLi6nLAN-8-) and put it into `data/preprocessed/` folder.\n",
    "1) download [inference_n3.tsv](https://drive.google.com/drive/folders/1uu_QIRIc4snbwdyl5w3H8DEPCKSG-LZT) and put it into `data/moral_stories_dataset/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data=pd.read_csv(OUTPUT2_PATH)\n",
    "\n",
    "segment_transform=[]\n",
    "skip=0\n",
    "for i in tqdm(range(len(loaded_data)), desc='context transform',mininterval=0.01):    \n",
    "    segment_char=loaded_data['segment_char'][i]\n",
    "    segment_action=loaded_data['segment_action'][i]\n",
    "    segment=loaded_data['segment'][i]\n",
    "    \n",
    "    if \"no action\" in str(segment_action) :\n",
    "        print(f'skip: {segment_action}')\n",
    "        skip+=1\n",
    "        segment_transform.append('not exist')\n",
    "    else:\n",
    "        output=context_extraction(segment_char, segment_action, segment)\n",
    "        segment_transform.append(output)\n",
    "\n",
    "assert len(loaded_data)==len(segment_transform)\n",
    "\n",
    "print(f\"skip: {skip}\")\n",
    "loaded_data['transform']=segment_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post preprocessing\n",
    "\n",
    "situation=[]\n",
    "intention=[]\n",
    "consequence=[]\n",
    "\n",
    "for i in tqdm(range(len(loaded_data))):\n",
    "    tmp=loaded_data['transform'][i]\n",
    "    tmp=tmp.strip('<start>').strip('<end>').replace('\"','').split('\\n')\n",
    "    \n",
    "    for k in range(len(tmp)):\n",
    "        if (len(tmp[k])>2) and (\":\" in tmp[k]):\n",
    "            split=tmp[k].split(':')\n",
    "            tag=split[0].strip('[').strip(']')\n",
    "            if tag.lower()=='situation':\n",
    "                situation.append(split[1])\n",
    "            elif tag.lower()=='intention':\n",
    "                intention.append(split[1])\n",
    "            elif tag.lower()=='consequence':\n",
    "                consequence.append(split[1])\n",
    "        \n",
    "    if len(situation)!=i+1 : situation.append('not exist')\n",
    "    if len(intention)!=i+1 : intention.append('not exist')\n",
    "    if len(consequence)!=i+1 : consequence.append('not exist')\n",
    "        \n",
    "    print(i, len(situation), len(intention), len(consequence))\n",
    "    assert len(situation)==len(intention)==len(consequence)\n",
    "\n",
    "loaded_data['intention'], loaded_data['situation'], loaded_data['consequence'] = intention, situation, consequence\n",
    "\n",
    "# Drop the 'transform' column and rename 'segment_action' to 'action'\n",
    "loaded_data = loaded_data.drop(columns=['transform'])\n",
    "loaded_data = loaded_data.rename(columns={'segment_action': 'action'})\n",
    "loaded_data['intention']=loaded_data['intention'].str.replace('[','').str.replace(']','')\n",
    "loaded_data['situation']=loaded_data['situation'].str.replace('[','').str.replace(']','')\n",
    "loaded_data['consequence']=loaded_data['consequence'].str.replace('[','').str.replace(']','')\n",
    "\n",
    "def mask_characters(row, column, char_to_mask):\n",
    "    mask = '[mask]'\n",
    "    if pd.notnull(row[column]):\n",
    "        return re.sub(char_to_mask, mask, str(row[column]), flags=re.IGNORECASE)\n",
    "    return row[column]\n",
    "\n",
    "for i in tqdm(range(len(loaded_data))):\n",
    "    char_to_mask = re.escape(str(loaded_data.at[i, 'segment_char']))\n",
    "    loaded_data.at[i, 'action'] = mask_characters(loaded_data.iloc[i], 'action', char_to_mask)\n",
    "    loaded_data.at[i, 'intention'] = mask_characters(loaded_data.iloc[i], 'intention', char_to_mask)\n",
    "    loaded_data.at[i, 'situation'] = mask_characters(loaded_data.iloc[i], 'situation', char_to_mask)\n",
    "    loaded_data.at[i, 'consequence'] = mask_characters(loaded_data.iloc[i], 'consequence', char_to_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save path for morality labeling using fine-tuned MAD\n",
    "loaded_data.to_csv(\"data/preprocessed/inference_n3.tsv\", sep='\\t', index=True)\n",
    "loaded_data.to_csv(OUTPUT2_PATH, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Morality Prediction Using MAD\n",
    "\n",
    "- MAD fine-tuning\n",
    "\n",
    "    ```\n",
    "    bash train_cls.sh\n",
    "    ```\n",
    "\n",
    "- Inference\n",
    "\n",
    "    ```\n",
    "    bash test_cls.sh\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled dataset path\n",
    "loaded_data=pd.read_csv(FINAL_PATH)\n",
    "\n",
    "# morality labeling\n",
    "label=[]\n",
    "for i in range(len(loaded_data)):\n",
    "    if \"no action\" in loaded_data['action'][i]:\n",
    "        label.append(-1)\n",
    "    else:\n",
    "        if loaded_data['SICA'][i]>=0.5: label.append(1)\n",
    "        else: label.append(0)\n",
    "\n",
    "loaded_data['label']=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Valid, Test split\n",
    "train=loaded_data[:101686].reset_index(drop=True)\n",
    "valid=loaded_data[101686:115046].reset_index(drop=True)\n",
    "test=loaded_data[115046:].reset_index(drop=True)\n",
    "\n",
    "print(len(train), len(valid), len(test)) # 101686 13360 12255\n",
    "print(len(train) + len(valid) + len(test)) # 127301\n",
    "\n",
    "train['split']=['train']*len(train)\n",
    "valid['split']=['valid']*len(valid)\n",
    "test['split']=['test']*len(test)\n",
    "\n",
    "data=pd.concat([train, valid, test])\n",
    "data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(FINAL_PATH, index=False)\n",
    "train.to_csv('data/movie_dataset/movie_dataset_train.csv', index=True)\n",
    "valid.to_csv('data/movie_dataset/movie_dataset_valid.csv', index=True)\n",
    "test.to_csv('data/movie_dataset/movie_dataset_test.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
